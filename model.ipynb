{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcmWFKdmo9B4",
        "outputId": "6a1b9108-01c2-491a-f470-45639ca2a240"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Z3Q10_VVlKvK",
        "outputId": "b0fa50df-1710-4e5a-ec74-4c734f3c407c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/facial-rating-cnns/data/archive.zip to /content/drive/MyDrive/facial-rating-cnns/data/...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/facial-rating-cnns/data/archive.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3594391555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Force Extraction (Ignore if folder exists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracting {zip_path} to {extract_path}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extraction finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/facial-rating-cnns/data/archive.zip'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/facial-rating-cnns/data/archive.zip'\n",
        "extract_path = '/content/drive/MyDrive/facial-rating-cnns/data/'\n",
        "\n",
        "# 1. Force Extraction (Ignore if folder exists)\n",
        "print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Extraction finished.\")\n",
        "\n",
        "# 2. Debug: List what is actually in the folder\n",
        "print(\"\\n--- Folder Contents ---\")\n",
        "files = os.listdir(extract_path)\n",
        "for f in files:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# 1. Define Paths\n",
        "# Source: Where your zip is right now (in Drive)\n",
        "drive_zip_path = '/content/drive/MyDrive/facial-rating-cnns/data/archive.zip'\n",
        "\n",
        "# Destination: Local Colab Machine (Super fast I/O)\n",
        "local_zip_path = '/content/archive.zip'\n",
        "local_extract_path = '/content/fast_data'\n",
        "\n",
        "# 2. Copy and Unzip Locally\n",
        "print(f\"Copying data from Drive to Local Disk... (This fixes the lag)\")\n",
        "shutil.copy(drive_zip_path, local_zip_path)\n",
        "\n",
        "print(\"Unzipping locally...\")\n",
        "if not os.path.exists(local_extract_path):\n",
        "    os.makedirs(local_extract_path)\n",
        "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(local_extract_path)\n",
        "\n",
        "print(\"Done! Data is now on local disk.\")\n",
        "\n",
        "# 3. UPDATE PATHS for Data Loading\n",
        "# We overwrite the variables so you don't need to change your other code\n",
        "base_dir = local_extract_path\n",
        "image_dir = os.path.join(base_dir, 'Images')\n",
        "label_path = os.path.join(base_dir, 'labels.txt')\n",
        "\n",
        "print(f\"New fast image dir: {image_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6olugdkwrFli",
        "outputId": "46aacd9c-7458-4fb3-d11c-b21d01d8866f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data from Drive to Local Disk... (This fixes the lag)\n",
            "Unzipping locally...\n",
            "Done! Data is now on local disk.\n",
            "New fast image dir: /content/fast_data/Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. Define Correct Paths (Based on your os.listdir) ---\n",
        "base_dir = '/content/fast_data'\n",
        "image_dir = os.path.join(base_dir, 'Images/Images')\n",
        "label_path = os.path.join(base_dir, 'labels.txt')\n",
        "\n",
        "print(f\"Checking label file at: {label_path}\")\n",
        "\n",
        "# --- 2. Load Labels ---\n",
        "try:\n",
        "    # Read the file to inspect formatting\n",
        "    with open(label_path, 'r') as f:\n",
        "        print(f\"First line of file: {f.readline().strip()}\")\n",
        "\n",
        "    # Load dataframe (Assuming standard format: ImageName Rating)\n",
        "    # We try comma first, then space if that fails, common in Kaggle re-uploads\n",
        "    try:\n",
        "        df = pd.read_csv(label_path, sep=',', header=None, names=['Image', 'Rating'])\n",
        "        # If the first column looks like a float/int, it might be the rating (bad parse), check naming\n",
        "        if not str(df.iloc[0, 0]).endswith(('.jpg', '.jpeg', '.png')):\n",
        "             df = pd.read_csv(label_path, sep='\\s+', header=None, names=['Image', 'Rating'])\n",
        "    except:\n",
        "        df = pd.read_csv(label_path, sep='\\s+', header=None, names=['Image', 'Rating'])\n",
        "\n",
        "    # Filter out any header rows if they exist (e.g. if the file has \"Image,Rating\" as text)\n",
        "    if not str(df.iloc[0, 1]).replace('.','',1).isdigit():\n",
        "        df = pd.read_csv(label_path, sep=None, engine='python') # Auto-detect header\n",
        "\n",
        "    print(f\"Success! Loaded {len(df)} labels.\")\n",
        "    print(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR reading labels: {e}\")\n",
        "\n",
        "# --- 3. Dataset Class ---\n",
        "class FaceBeautyDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(self.dataframe.iloc[idx, 0])\n",
        "        # Fix for some dataset versions that might have paths in the filename\n",
        "        if '/' in img_name or '\\\\' in img_name:\n",
        "            img_name = os.path.basename(img_name)\n",
        "\n",
        "        full_img_path = os.path.join(self.root_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(full_img_path).convert('RGB')\n",
        "        except:\n",
        "            image = Image.new('RGB', (224, 224)) # Black placeholder\n",
        "\n",
        "        # Ensure rating is float\n",
        "        rating = float(self.dataframe.iloc[idx, 1])\n",
        "        rating = torch.tensor(rating, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, rating\n",
        "\n",
        "# --- 4. Create Loaders ---\n",
        "if 'df' in locals():\n",
        "    # Transforms\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_dataset = FaceBeautyDataset(train_df, image_dir, transform=data_transforms['train'])\n",
        "    val_dataset = FaceBeautyDataset(val_df, image_dir, transform=data_transforms['val'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"Data Loaders Ready. Proceed to Training.\")\n",
        "else:\n",
        "    print(\"Fix the label loading error above first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ypyedIWlmFt",
        "outputId": "a4b98bac-1888-4835-ac9e-014d08eabe5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking label file at: /content/fast_data/labels.txt\n",
            "First line of file: CF437.jpg 2.883333\n",
            "Success! Loaded 5500 labels.\n",
            "        Image    Rating\n",
            "0   CF437.jpg  2.883333\n",
            "1  AM1384.jpg  2.466667\n",
            "2  AM1234.jpg  2.150000\n",
            "3  AM1774.jpg  3.750000\n",
            "4   CF215.jpg  3.033333\n",
            "Data Loaders Ready. Proceed to Training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:28: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:30: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:30: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-2639910264.py:28: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  df = pd.read_csv(label_path, sep='\\s+', header=None, names=['Image', 'Rating'])\n",
            "/tmp/ipython-input-2639910264.py:30: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  df = pd.read_csv(label_path, sep='\\s+', header=None, names=['Image', 'Rating'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import time\n",
        "import copy\n",
        "\n",
        "def train_model(model, criterion, optimizer, num_epochs=20):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on: {device}\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    preds = outputs.squeeze()\n",
        "                    loss = criterion(preds, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'best_face_rater_colab.pth')\n",
        "\n",
        "    print(f'Best val Loss: {best_loss:.4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Initialize and Train\n",
        "model = models.resnet18(weights='DEFAULT')\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "trained_model = train_model(model, criterion, optimizer, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbsDjG2dlpQr",
        "outputId": "b4c7ddbd-eaa3-4329-a61b-d6ca274edab6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda\n",
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 0.8945\n",
            "val Loss: 0.1280\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.0984\n",
            "val Loss: 0.1835\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.0820\n",
            "val Loss: 0.1022\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.0586\n",
            "val Loss: 0.1166\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.0507\n",
            "val Loss: 0.1083\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.0414\n",
            "val Loss: 0.1167\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.0367\n",
            "val Loss: 0.0940\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.0348\n",
            "val Loss: 0.0950\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.0288\n",
            "val Loss: 0.0910\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.0291\n",
            "val Loss: 0.1598\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.0242\n",
            "val Loss: 0.0933\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.0188\n",
            "val Loss: 0.0858\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.0195\n",
            "val Loss: 0.0883\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.0212\n",
            "val Loss: 0.0932\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.0193\n",
            "val Loss: 0.0912\n",
            "Best val Loss: 0.0858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "def predict_beauty_score(image_path, model_path='best_face_rater_colab.pth'):\n",
        "    # 1. Setup Device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 2. Recreate Model Architecture\n",
        "    model = models.resnet18(weights=None)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "    # 3. Load Weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 4. Transform Image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 5. Predict\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        score = output.item()\n",
        "\n",
        "    return score\n",
        "\n",
        "# --- RUN IT ---\n",
        "# Change 'test.jpg' to the name of the file you uploaded\n",
        "img_name = 'test.jpg'\n",
        "\n",
        "# Simple check to avoid crash if you haven't uploaded yet\n",
        "import os\n",
        "if os.path.exists(img_name):\n",
        "    score = predict_beauty_score(img_name)\n",
        "    print(f\"Predicted Rating: {score:.2f} / 5.0\")\n",
        "else:\n",
        "    print(f\"Please upload an image named '{img_name}' to the Colab files sidebar first!\")"
      ],
      "metadata": {
        "id": "nwEa4MZGlrPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ceba745-08f8-4d94-9959-bb3f4bdcb948"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Rating: 3.66 / 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgXHqbAptSZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}